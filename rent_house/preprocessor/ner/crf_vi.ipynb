{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi.pyvi import ViTokenizer,ViPosTagger\n",
    "\n",
    "def readFile_txt(filename):\n",
    "    data=[]\n",
    "    f = open(filename)\n",
    "#     next(f)\n",
    "    for row in f:\n",
    "        data.append(row)\n",
    "    return data\n",
    "\n",
    "def make_data_train(data):\n",
    "    data_train=[]\n",
    "    for row in data:\n",
    "        post_tup=ViPosTagger.postagging(ViTokenizer.tokenize(row))\n",
    "        post_list=tupToList(post_tup)\n",
    "        data_train.append(post_list)\n",
    "    return data_train\n",
    "\n",
    "def writeListFile(filename,data):\n",
    "    str_out=''\n",
    "    for item in data:\n",
    "        for tup in item:\n",
    "            str_out+=tup[0]+\" \"+tup[1]+\" \"+tup[2]+\"\\n\"\n",
    "        str_out+=\"\\n\"\n",
    "#     print(str_out)\n",
    "    F = open(filename,\"w\")\n",
    "    F.write(str(str_out))\n",
    "def tupToList(tup):\n",
    "    list1=[]\n",
    "    for item in tup[0]:\n",
    "        index=tup[0].index(item)\n",
    "        tup_=(item,tup[1][index],'0')\n",
    "        list1.append(tup_)\n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tạo file tiền huấn luyện\n",
    "data=readFile_txt(\"train1\")\n",
    "data_train=make_data_train(data)\n",
    "# print(len(data_train))\n",
    "# print(data_train)\n",
    "writeListFile(\"train1.pre\",data_train)\n",
    "# tạo file test tiền huấn luyện\n",
    "data=readFile_txt(\"test1\")\n",
    "data_test=make_data_train(data)\n",
    "# print(len(data_test))\n",
    "# print(data_test)\n",
    "writeListFile(\"test1.pre\",data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Trường', 'N', '0'),\n",
       " ('đại_học', 'N', '0'),\n",
       " ('Bách_Khoa', 'Np', '0'),\n",
       " ('Hà_Nội', 'Np', '0')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tup1=ViPosTagger.postagging(ViTokenizer.tokenize(u\"Trường đại học Bách Khoa Hà Nội\"))\n",
    "# tup1[0]\n",
    "# tup1[1]\n",
    "tupToList(tup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# create data train\n",
    "data_train=readFile_txt(\"train1.pre\")\n",
    "data_test=readFile_txt(\"test1.pre\")\n",
    "def getMatrix(data):\n",
    "    matrix=[]\n",
    "    vector=[]\n",
    "    line=0\n",
    "    for dt in data:\n",
    "        if len(dt)>1:\n",
    "            dt = dt[:-1]\n",
    "            l1=dt.split(\" \")\n",
    "            t1=(l1[0],l1[1],l1[2])\n",
    "            vector.append(t1)\n",
    "        else:\n",
    "            matrix.append(vector)\n",
    "            vector=[]\n",
    "    print (len(matrix))\n",
    "    return matrix\n",
    "matrix_train=getMatrix(data_train)\n",
    "matrix_test=getMatrix(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'+1:postag': 'N',\n",
       " '+1:postag[:2]': 'N',\n",
       " '+1:word.istitle()': False,\n",
       " '+1:word.isupper()': False,\n",
       " '+1:word.lower()': 'giá',\n",
       " 'BOS': True,\n",
       " 'bias': 1.0,\n",
       " 'postag': 'C',\n",
       " 'postag[:2]': 'C',\n",
       " 'word.isdigit()': False,\n",
       " 'word.istitle()': True,\n",
       " 'word.isupper()': False,\n",
       " 'word.lower()': 'còn',\n",
       " 'word[-2:]': 'òn',\n",
       " 'word[-3:]': 'Còn'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(matrix_train[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 0 ns, total: 20 ms\n",
      "Wall time: 21.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in matrix_train]\n",
    "y_train = [sent2labels(s) for s in matrix_train]\n",
    "\n",
    "X_test = [sent2features(s) for s in matrix_test]\n",
    "y_test = [sent2labels(s) for s in matrix_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48 ms, sys: 0 ns, total: 48 ms\n",
      "Wall time: 55 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "import pickle\n",
    "f = open('../model/ner.pkl', 'wb')\n",
    "pickle.dump(crf, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TYP', 'STR', 'CTY', 'WRD', 'PRV', 'PRC', 'PRJ', 'SQR']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(crf.classes_)\n",
    "labels.remove('0')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5952380952380952"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        SQR      0.000     0.000     0.000         0\n",
      "        PRC      0.000     0.000     0.000         1\n",
      "        WRD      0.000     0.000     0.000         0\n",
      "        PRJ      1.000     0.500     0.667         2\n",
      "        PRV      0.000     0.000     0.000         2\n",
      "        STR      1.000     0.500     0.667         2\n",
      "        CTY      1.000     1.000     1.000         3\n",
      "        TYP      1.000     0.500     0.667         4\n",
      "\n",
      "avg / total      0.786     0.500     0.595        14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
